\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\input{assignment.sty}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}


\lstdefinestyle{hpcblock}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!60},
    xleftmargin=1.5em,
    xrightmargin=1.5em,
    aboveskip=1em,
    belowskip=1em,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=hpcblock}

\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Paolo Deidda}{Discussed with: FULL NAME}{Solution for Project 4}{}
\newline

\assignmentpolicy

\tableofcontents
\newpage

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 1 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %

\section{Task: Ring maximum using MPI [10 Points]}

I detrmine each process message destination and source using its rank.

The communication can be implemented using the standard MPI functions 
\texttt{MPI\_Send} and \texttt{MPI\_Recv}, which take the following arguments:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item buffer address (to store sent or received data)
    \item number of elements (to send or receive)
    \item MPI datatype (of the elements to send or receive)
    \item destination (or source) rank (of the process to send to or receive from)
    \item message tag (to identify the message with an id)
    \item communicator (to identify the group of processes involved in the communication, useless in this case)
\end{itemize}

When using \texttt{MPI\_Send} and \texttt{MPI\_Recv} separately, the ranks must be split into two groups like even and odd ranks, otherwise all processes may block by attempting to send or receive at the same time.

Alternatively, we can use the \texttt{MPI\_Sendrecv} function, which takes care of both sending and receiving in a single call, avoiding deadlocks.
\lstinputlisting[
    language=C++,
    numbers=left,
    caption={Implemetation from file \href{../Skeleton_codes/ring/ring_sum.c}{ring\_sum.c}},
    captionpos=b,
    label={lst:mergeCritical},
    linerange={21-25},
    firstnumber=21
]{../Skeleton_codes/ring/ring_sum.c}


Once launched the job we can see the folloring output with \texttt{--ntasks=4} on the left and \texttt{--ntasks=8} on the right.

\begin{center}
\begin{minipage}[t]{0.45\textwidth}
    \lstinputlisting[
        language=C++,
        caption={Output from file \href{../Skeleton_codes/ring/ring_57313.out}{ring\_57313.out}},
        captionpos=b,
        label={lst:mergeCritical},
        linerange={3-6},
        firstnumber=3
    ]{../Skeleton_codes/ring/ring_57313.out}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
    \lstinputlisting[
        language=C++,
        caption={Output from file \href{../Skeleton_codes/ring/ring_57315.out}{ring\_57315.out}},
        captionpos=b,
        label={lst:mergeCritical},
        linerange={3-10},
        firstnumber=3
    ]{../Skeleton_codes/ring/ring_57315.out}
\end{minipage}
\end{center}

With flag \texttt{--ntasks=4} slurm automatically assign a node with at least 4 cores, since by default:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item 1 Slurm task = 1 MPI process
    \item 1 MPI process = 1 CPU core
\end{itemize}


 % -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 2 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Ghost cells exchange between neighboring processes [15 Points]}

I first create a $4 \times 4$ Cartesian communicator with periodic boundaries on both axes. Then, by using \texttt{MPI\_Cart\_shift}, I get the ranks of the neighboring processesnorth, south, east, and west, making sure the first and last ranks stay connected. To handle the column ghost layer, I create a derived datatype using \texttt{MPI\_Type\_vector}; this datatype covers one element for every \texttt{DOMAINSIZE} entries and goes all the way through the interior height of the tile.

I then keep using \texttt{MPI\_Sendrecv} calls, one for each direction, to avoid any deadlock. Each call sends the interior boundary (the rows or columns that leave out the corner points) and directly receives the data into the ghost layer.



\lstinputlisting[
    language=C,
    numbers=left,
    caption={Two example from top and bottom ghost cells exchange from the file \href{../Skeleton_codes/ghost/ghost.c}{ghost.c} with \texttt{SUBDOMAIN = 6}},
    captionpos=b,
    label={lst:ghostExchange},
    linerange={105-113},
    firstnumber=105
]{../Skeleton_codes/ghost/ghost.c}

Here the output of the program:


\lstinputlisting[
    language=C,
    caption={Output from file \href{../Skeleton_codes/ghost/ghost_57333.out}{ghost\_57333.out}},
    captionpos=b,
    label={lst:ghostExchange},
    linerange={3-11},
    firstnumber=3
]{../Skeleton_codes/ghost/ghost_57333.out}


% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 3 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Parallelizing the Mandelbrot set using MPI [20 Points]}

The function \texttt{createPartition} configure the process grid with \texttt{MPI\_Dims\_create}, then builds a Cartesian topology using the function \texttt{MPI\_Cart\_create}, and gets the coordinates for each process with \texttt{MPI\_Cart\_coords}. With this information, \texttt{createDomain} gives each process a rectangular subdomain, figuring out their global boundaries (\texttt{startx}, \texttt{endx}, \texttt{starty}, \texttt{endy}).

Next, each process calculates its own part of the Mandelbrot image independently. When they're done, the non-root processes send their sub-partition to the master using \texttt{MPI\_Send}, while the master collects all the blocks with \texttt{MPI\_Recv}, updates the partition through \texttt{updatePartition}, and places each block in the right spot in the final PNG.

The finished image is displayed in Figure~\ref{fig:mandel}.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mandel/mandel.png}
        \caption{Final Mandelbrot image computed in parallel (MPI).}
        \label{fig:mandel}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mandel/perf.pdf}
        \caption{Per-process computation time for varying MPI process counts.}
        \label{fig:mandel-perf}
    \end{subfigure}
    \caption{Mandelbrot set computed in parallel using MPI.}
    \label{fig:mandel and perf}
\end{figure}

In Figure~\ref{fig:mandel-perf}, we can see that as we increase the number of MPI processes, the computation time for each process goes down quite a bit. It scales very good up to \texttt{8} processes. But when you push to \texttt{16} processes, the workload is light enough that you start to notice some overhead and imbalance, which is pretty typical for domain-splitting algorithms.
The imbalance in workload distribution among precesses might be due to the static division of the image so the central processes end up with more complex calculations. This should be possible to resolve with a dynamic task allocation strategy but in OpenMP.

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 4 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Parallel matrix-vector multiplication and the power method [40 Points]}


% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Report Quality ------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task:  Quality of the Report [15 Points]}




\end{document}
