\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\input{assignment.sty}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}


\lstdefinestyle{hpcblock}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!60},
    xleftmargin=1.5em,
    xrightmargin=1.5em,
    aboveskip=1em,
    belowskip=1em,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=hpcblock}

\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Paolo Deidda}{Discussed with: Lino Candian}{Solution for Project 4}{}
\newline

\assignmentpolicy

\tableofcontents
\newpage

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 1 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %

\section{Task: Ring maximum using MPI [10 Points]}

I detrmine each process message destination and source using its rank.

The communication can be implemented using the standard MPI functions 
\texttt{MPI\_Send} and \texttt{MPI\_Recv}, which take the following arguments:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item buffer address (to store sent or received data)
    \item number of elements (to send or receive)
    \item MPI datatype (of the elements to send or receive)
    \item destination (or source) rank (of the process to send to or receive from)
    \item message tag (to identify the message with an id)
    \item communicator (to identify the group of processes involved in the communication, useless in this case)
\end{itemize}

When using \texttt{MPI\_Send} and \texttt{MPI\_Recv} separately, the ranks must be split into two groups like even and odd ranks, otherwise all processes may block by attempting to send or receive at the same time.

Alternatively, we can use the \texttt{MPI\_Sendrecv} function, which takes care of both sending and receiving in a single call, avoiding deadlocks.
\lstinputlisting[
    language=C++,
    numbers=left,
    caption={Implemetation from file \href{../Skeleton_codes/ring/ring_sum.c}{ring\_sum.c}},
    captionpos=b,
    label={lst:mergeCritical},
    linerange={21-25},
    firstnumber=21
]{../Skeleton_codes/ring/ring_sum.c}


Once launched the job we can see the folloring output with \texttt{--ntasks=4} on the left and \texttt{--ntasks=8} on the right.

\begin{center}
\begin{minipage}[t]{0.45\textwidth}
    \lstinputlisting[
        language=C++,
        caption={Output from file \href{../Skeleton_codes/ring/ring_57313.out}{ring\_57313.out}},
        captionpos=b,
        label={lst:mergeCritical},
        linerange={3-6},
        firstnumber=3
    ]{../Skeleton_codes/ring/ring_57313.out}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
    \lstinputlisting[
        language=C++,
        caption={Output from file \href{../Skeleton_codes/ring/ring_57315.out}{ring\_57315.out}},
        captionpos=b,
        label={lst:mergeCritical},
        linerange={3-10},
        firstnumber=3
    ]{../Skeleton_codes/ring/ring_57315.out}
\end{minipage}
\end{center}

With flag \texttt{--ntasks=4} slurm automatically assign a node with at least 4 cores, since by default:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item 1 Slurm task = 1 MPI process
    \item 1 MPI process = 1 CPU core
\end{itemize}


 % -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 2 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Ghost cells exchange between neighboring processes [15 Points]}

I first create a $4 \times 4$ Cartesian communicator with periodic boundaries on both axes. Then, by using \texttt{MPI\_Cart\_shift}, I get the ranks of the neighboring processesnorth, south, east, and west, making sure the first and last ranks stay connected. To handle the column ghost layer, I create a derived datatype using \texttt{MPI\_Type\_vector}; this datatype covers one element for every \texttt{DOMAINSIZE} entries and goes all the way through the interior height of the tile.

I then keep using \texttt{MPI\_Sendrecv} calls, one for each direction, to avoid any deadlock. Each call sends the interior boundary (the rows or columns that leave out the corner points) and directly receives the data into the ghost layer.



\lstinputlisting[
    language=C,
    numbers=left,
    caption={Two example from top and bottom ghost cells exchange from the file \href{../Skeleton_codes/ghost/ghost.c}{ghost.c} with \texttt{SUBDOMAIN = 6}},
    captionpos=b,
    label={lst:ghostExchange},
    linerange={105-113},
    firstnumber=105
]{../Skeleton_codes/ghost/ghost.c}

Here the output of the program:


\lstinputlisting[
    language=C,
    caption={Output from file \href{../Skeleton_codes/ghost/ghost_57333.out}{ghost\_57333.out}},
    captionpos=b,
    label={lst:ghostExchange},
    linerange={3-11},
    firstnumber=3
]{../Skeleton_codes/ghost/ghost_57333.out}


% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 3 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Parallelizing the Mandelbrot set using MPI [20 Points]}

The function \texttt{createPartition} configure the process grid with \texttt{MPI\_Dims\_create}, then builds a Cartesian topology using the function \texttt{MPI\_Cart\_create}, and gets the coordinates for each process with \texttt{MPI\_Cart\_coords}. With this information, \texttt{createDomain} gives each process a rectangular subdomain, figuring out their global boundaries (\texttt{startx}, \texttt{endx}, \texttt{starty}, \texttt{endy}).

Next, each process calculates its own part of the Mandelbrot image independently. When they're done, the non-root processes send their sub-partition to the master using \texttt{MPI\_Send}, while the master collects all the blocks with \texttt{MPI\_Recv}, updates the partition through \texttt{updatePartition}, and places each block in the right spot in the final PNG.

The finished image is displayed in Figure~\ref{fig:mandel}.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mandel/mandel.png}
        \caption{Final Mandelbrot image computed in parallel (MPI).}
        \label{fig:mandel}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mandel/perf.pdf}
        \caption{Per-process computation time for varying MPI process counts.}
        \label{fig:mandel-perf}
    \end{subfigure}
    \caption{Mandelbrot set computed in parallel using MPI.}
    \label{fig:mandel and perf}
\end{figure}

In Figure~\ref{fig:mandel-perf}, we can see that as we increase the number of MPI processes, the computation time for each process goes down quite a bit. It scales very good up to \texttt{8} processes. But when you push to \texttt{16} processes, the workload is light enough that you start to notice some overhead and imbalance, which is pretty typical for domain-splitting algorithms.
The imbalance in workload distribution among precesses might be due to the static division of the image so the central processes end up with more complex calculations. This should be possible to resolve with a dynamic task allocation strategy but in OpenMP.

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 4 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Parallel matrix-vector multiplication and the power method [40 Points]}

The given skeleton file, \texttt{powermethod\_rows.c}, already took care of the serial part of the power iteration. I implemented spreading the matrix among the MPI processes, sending out the iterate vector, and combining the partial results to refine the eigenvalue estimate.

The MPI routines used were:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item \texttt{MPI\_Scatterv}: distribute each block of matrix rows to its process
    \item \texttt{MPI\_Bcast}: broadcast the current iterate vector $x$ to all processes
    \item \texttt{MPI\_Reduce}: compute global inner products required by the power iteration
    \item \texttt{MPI\_Gather}: collect the updated vector entries on the master process
\end{itemize}

To run all required experiments, I used four automated Bash:

\lstinputlisting[
    language=bash,
    numbers=left,
    caption={Bash script to run all experiments \href{../Skeleton\_codes/powermethod/run\_all.sh}{run\_all.sh}},
    captionpos=b,
    label={lst:runAll},
    linerange={3-6},
    firstnumber=1
]{../Skeleton_codes/powermethod/run_all.sh}

The resulting strong- and weak-scaling plots are shown in Figure~\ref{fig:powermethod-results}.  
Each of these plots shows the runtime and efficiency curves for both single-node and multi-node setups.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/powermethod/strong_scaling_combined.png}
        \caption{Strong scaling: runtime and efficiency.}
        \label{fig:strong-scaling}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/powermethod/weak_scaling_combined.png}
        \caption{Weak scaling: runtime and efficiency.}
        \label{fig:weak-scaling}
    \end{subfigure}
    \caption{Strong and weak scaling results for the power method.}
    \label{fig:powermethod-results}
\end{figure}

\textbf{Strong Scaling: \ref{fig:strong-scaling}} \\ 
The strong scaling results show that parallelization—runtimes keep decreasing as we increase the number of processes. The multi-node setup (in orange) maintains over 80\% efficiency throughout the tests. On the other hand, the single-node setup (in blue) has been pretty unstable, especially with 9 and 17 processes, where we see a significant drop in efficiency. This difference points out that, while the algorithm logically scales well, running too many processes onto a single node hits some hardware limits, probably due to memory bandwidth issues that instead are not present when we spread the load over multiple nodes.

\textbf{Weak Scaling: \ref{fig:weak-scaling}} \\
The weak scaling graph shows we’re close to ideal performance for the multi-node setup. The runtime for the multi-node system (orange solid line) stays nearly flat, and the efficiency (orange dashed line) stays pretty close to 1.0, which shows that we’re not adding much overhead as we scale both the problem size and the number of processors. In contrast, the single-node performance (in blue) have intability spikes in runtime and drops in efficiency. This really confirms that a single machine can’t handle the total memory bandwidth needed as the problem size grows, underscoring why a multi-node approach is necessary.

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Report Quality ------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task:  Quality of the Report [15 Points]}




\end{document}
