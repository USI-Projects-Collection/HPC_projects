\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}

\lstdefinestyle{hpcblock}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!60},
    xleftmargin=1.5em,
    xrightmargin=1.5em,
    aboveskip=1em,
    belowskip=1em,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=hpcblock}


\input{assignment.sty}

\begin{document}

\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Paolo Deidda}{Discussed with: Lino Candian}{Solution for Project 5}{}
\newline

\assignmentpolicy

\tableofcontents
\newpage

% =============================================================================
\section{Parallel Space Solution of a nonlinear PDE using MPI [in total 60 points]}
% =============================================================================

Image solution for this exercise can seen in Figure~\ref{fig:ex1}.

\subsection{Initialize/finalize MPI and welcome message [5 Points]}
Output:
\lstinputlisting[
    language=bash,
    % numbers=left,
    % caption={Welcome message update in main.cpp},
    captionpos=b,
    label={lst:mergeCritical},
    % linerange={0},
    % firstnumber=111
]{../Skeleton_codes/mini_app/out_ex1_1.txt}

\subsection{Domain decomposition [10 Points]}

I use a two–dimensional Cartesian domain decomposition:

\begin{itemize}
    \item \textbf{Process grid creation.}  
    I obtain a balanced $d_x \times d_y$ grid using \texttt{MPI\_Dims\_create}, which guarantees that the process grid is as close to square as possible for any number of processes~$P$.

    \item \textbf{Cartesian topology.}  
    I build the topology with \texttt{MPI\_Cart\_create} so that each process can identify its coordinates and its four neighbors through the apposite \texttt{MPI\_Cart\_coords} and \texttt{MPI\_Cart\_shift} functions.

    \item \textbf{Subdomain sizes.}  
    The global domain $n \times n$ is partitioned into rectangular subdomains.  
    Local sizes $(n_x, n_y)$ are computed from $(d_x, d_y)$, and any remainder is distributed across the first processes in each direction so that subdomain sizes differ by at most one cell.

    \item \textbf{Load balancing.}  
    With this decomposition all processes receive almost the same number of grid points $N = n_x n_y$.

    \item \textbf{Communication overhead.}  
    In a 2D layout, the amount of halo data exchanged with neighbors grows only as $\mathcal{O}(n_x + n_y)$, while work grows as $\mathcal{O}(n_x n_y)$.  
    This results in a lower communication-to-computation ratio than a 1D decomposition.
\end{itemize}

\subsection{Linear algebra kernels [5 Points]}

Only two kernels required MPI parallelization:

\begin{itemize}
    \item \textbf{hpc\_dot}: computes a global inner product.  
    I added \texttt{MPI\_Allreduce} to combine the partial sums from all ranks.

    \item \textbf{hpc\_norm2}: computes the global 2-norm.  
    I used \texttt{MPI\_Allreduce} to sum the local squared values before taking the square root.
\end{itemize}

All other \texttt{hpc\_xxx} functions operate purely on local data (vector updates, axpy, copy, scale) and therefore do not require communication or any MPI modification.

\subsection{The diffusion stencil: Ghost cells exchange [10 Points]}

I implemented the ghost-cell exchange using \textbf{non-blocking point-to-point communication} so that communication and computation can overlap.  
The steps and MPI calls used are:

\begin{itemize}
    \item \textbf{Packing send buffers.}  
    Before communication, each process copies its local boundary data into four buffers: \texttt{buffN}, \texttt{buffS}, \texttt{buffE}, \texttt{buffW}.

    \item \textbf{Posting non-blocking receives: \texttt{MPI\_Irecv}.}  
    For every existing neighbor (north, south, east, west), I post an \texttt{MPI\_Irecv} on the corresponding receive buffer (\texttt{bndN}, \texttt{bndS}, \texttt{bndE}, \texttt{bndW}).  
    Since \texttt{MPI\_Irecv} is non-blocking, control returns immediately without waiting for the message.

    \item \textbf{Posting non-blocking sends: \texttt{MPI\_Isend}.}  
    After the receives, each process posts the matching \texttt{MPI\_Isend} using the packed buffers.  
    Being non-blocking, these sends also do not stop execution.

    \item \textbf{Overlap of communication and computation.}  
    Thanks to the non-blocking operations, the stencil computation on all \textbf{interior points} (i.e.\ grid points that do not require ghost data) can proceed while data is in transit.  
    Only boundary points depend on the incoming ghost values, so they are computed later.

    \item \textbf{Waiting for completion: \texttt{MPI\_Waitall}.}  
    After finishing the interior region, all pending communications are completed with a single \texttt{MPI\_Waitall}.  
    Once the ghost buffers are updated, the stencil is applied safely to the boundary and corner points.
\end{itemize}

With blocking functions like \texttt{MPI\_Send}, \texttt{MPI\_Recv}, or \texttt{MPI\_Sendrecv}, every process has to wait for its neighbors to finish before it can move on, which really slows things down. But by using \texttt{MPI\_Isend}, \texttt{MPI\_Irecv}, and \texttt{MPI\_Waitall}, the solver can work on the calculations while it's waiting for communication, which helps boost efficiency.


\subsection{Implement parallel I/O [10 Points]}

In the \texttt{write\_binary} function, I handle parallel output using MPI-I/O file views. Instead of figuring out byte offsets for each row by hand, I create a derived datatype that aligns the local subdomain straight onto the global grid structure.

The main feature here is \texttt{MPI\_Type\_create\_subarray}, which sets up a non-contiguous data layout. With the global dimensions of ($N \times N$) and the local coordinates ($\text{start}_x, \text{start}_y$), MPI takes care of calculating the displacement in the file automatically. In simple terms, this determines where we'll write in the global offset:
\[
\text{Offset} = (\text{start}_y \times N_{\text{global}} + \text{start}_x) \times \text{sizeof(double)}
\]

By using this datatype with \texttt{MPI\_File\_set\_view}, processes can call \texttt{MPI\_File\_write\_all} to write their whole block together. This setup lets the MPI library fine-tune disk access while keeping the global order of the matrix intact. As a result, we end up with one binary array that holds the complete global solution, eliminating the need for any extra processing or temporary files.

\newpage
\subsection{Strong scaling [10 Points]}
Higher resulution images can be found in their respective folders in \href{../Skeleton\_codes/mini\_app/}{mini\_app\/}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../project_3/Skeleton_codes/mini_app/strong_scaling/strong_scaling_time_vs_threads.png}
        % \caption{openMP (project 3)}
        \label{fig:lowres}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/strong_scaling/strong_scaling_time_vs_threads.png}
        % \caption{MPI multi task per node}
        \label{fig:lowres}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/strong_scaling_muliti_node/strong_scaling_time_vs_threads.png}
        % \caption{MPI single task per node}
        \label{fig:lowres}
    \end{subfigure}

    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../project_3/Skeleton_codes/mini_app/strong_scaling/strong_scaling_speedup.png}
        \caption{openMP (project 3)}
        \label{fig:a1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/strong_scaling/strong_scaling_speedup.png}
        \caption{MPI multi task over 1 node}
        \label{fig:b1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/strong_scaling_muliti_node/strong_scaling_speedup.png}
        \caption{MPI 1 task over multiple nodes}
        \label{fig:c1}
    \end{subfigure}
    \caption{\textit{Strong} scaling results: \textbf{Above} time over number of threads - \textbf{Below} efficiency over number of threads.}
    \label{fig:strong_scaling}
\end{figure}

Figure~\ref{fig:strong_scaling} shows the results for strong scaling, comparing three setups: the OpenMP implementation \ref{fig:a1} MPI on a single node \ref{fig:b1}, and MPI spread across multiple nodes \ref{fig:c1}. The top row presents the time it took to reach a solution, while the bottom row illustrates the speedup ($T_1/T_N$).

\textbf{Comparison with OpenMP (Single Node):}
Looking at MPI on a single node: The MPI setup running on one node \ref{fig:b1} has a scaling profile that's pretty similar to the OpenMP version \ref{fig:a1}. When we work with larger grid sizes ($n=1024$), both setups manage to achieve almost linear speedup up to 16 processes, with MPI showing a slight edge in performance (reaching a speedup of $\approx 13\times$ vs. $\approx 11\times$ for OpenMP). This indicates that the overhead from MPI's internal memory copying either compares or is better managed than the thread synchronization that come with the OpenMP shared-memory model.

\textbf{Distributed Memory Performance (Multi-Node):}
The multi-node MPI configuration (c) highlights the critical impact of network latency. 
\begin{itemize}
    \item \textbf{Large Grids ($n=1024$):} The computations scale impressively, hitting the highest linearity among the three cases tested. The effective computation-to-communication ratio does a great job of masking the overhead from the network.
    \item \textbf{Small Grids ($n \le 128$):} Performance takes a significant hit. When $n=64$, the speedup drops below 1.0 right away, indicating that parallel execution ends up slower than doing it serially. This happens because the sub-domains are too small; the fixed costs associated with network latency and halo exchanges overshadow the small amount of computational work available per rank.
\end{itemize}

In summary, while MPI performs just as well as OpenMP within a single node, its scalability across multiple nodes really rely on the size of the problem. Distributed parallelism proves advantageous only when the local domain size is large enough to offset the costs associated with communication between nodes.


\newpage
\subsection{Weak scaling [10 Points]}
Higher resulution images can be found in their respective folders in \href{../Skeleton\_codes/mini\_app/}{mini\_app\/}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../project_3/Skeleton_codes/mini_app/weak_scaling/weak_scaling_time_vs_threads.png}
        % \caption{openMP (project 3)}
        \label{fig:lowres}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/weak_scaling/weak_scaling_time_vs_threads.png}
        % \caption{MPI multi task per node}
        \label{fig:lowres}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/weak_scaling_multi_node/weak_scaling_time_vs_threads.png}
        % \caption{MPI single task per node}
        \label{fig:lowres}
    \end{subfigure}

    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../project_3/Skeleton_codes/mini_app/weak_scaling/weak_scaling_efficiency.png}
        \caption{openMP (project 3)}
        \label{fig:a2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/weak_scaling/weak_scaling_efficiency.png}
        \caption{MPI multi task over 1 node}
        \label{fig:b2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/weak_scaling_multi_node/weak_scaling_efficiency.png}
        \caption{MPI 1 task over multiple nodes}
        \label{fig:c2}
    \end{subfigure}
    \caption{\textit{Weak} scaling results: \textbf{Above} time over number of threads - \textbf{Below} efficiency over number of threads.}
    \label{fig:weak_scaling}
\end{figure}

Figure~\ref{fig:weak_scaling} shows how weak scaling performs when the problem size grows along with the number of processing elements ($N$), which helps keep a consistent workload for each process. Ideally, we want the time it takes to reach a solution to stay the same as a flat line; an efficiency of  $\approx 1$.

\textbf{Single-Node Bottlenecks (OpenMP vs. MPI):}
Both the OpenMP implementation \ref{fig:a2} and the single-node MPI run \ref{fig:b2} show a notable drop from the ideal weak scaling. As $N$ increases, the runtime goes up steadily, causing the efficiency to drop below 0.4 at $N=16.$ 
This suggests that the system is \textbf{memory-bound}. Even though the computational workload per core stays constant, the total memory bandwidth needed increases with N. Since all processes on a single node are competing for the same shared memory bus, the bandwidth fills up, leading to a bottleneck that slows down all processes at once. The similarity between \ref{fig:a2} and \ref{fig:b2} indicates that this hardware limitation impacts both parallel models equally.

\textbf{Multi-Node Scaling and Network Overhead:}
The multi-node configuration \ref{fig:c2} presents a dual behavior dependent on the base resolution:
\begin{itemize}
    \item \textbf{Small Base ($n=64$):} 
    The performance is quite poor here. Right off the bat, the efficiency drops to $\approx 0.1$ immediately. The local sub-domain is just too small to make the fixed overhead of network latency (inter-node communication) worthwhile, which results in execution being limited by latency.
    \item \textbf{Large Base ($n=256$):} 
    Interestingly, the multi-node configuration performs better than the single-node setup. The runtime slope for the green line in \ref{fig:c2} is less steep than in \ref{fig:b2}. This is because spreading tasks across multiple nodes means access to combined memory bandwidth, as each node has its own memory bus. Unlike the single-node scenario, the computation isn't held back by memory contention, provided the local problem size is large enough to offset the costs of network communication.
\end{itemize}

In conclusion, while single-node scaling is held back by hardware memory bandwidth, multi-node scaling is mainly limited by network latency. So, the multi-node approach is definitely better for large-scale problems that need high memory throughput.



\newpage
% =============================================================================
\section{Python for High-Performance Computing [in total 25 points]}
% =============================================================================

\subsection{Sum of ranks: MPI collectives [5 Points]}

Two different communication approaches: generic \textit{object serialization} vs \textit{direct buffer handling}.

\begin{itemize}
    \item \textbf{Pickle-based communication (\texttt{sum\_ranks\_pickle.py}):} 
    I used the lowercase \texttt{comm.allreduce} method, which lets us send any Python objects, like simple integers. However, it needs the \texttt{pickle} module for both serialization and deserialization. While it offers great flexibility, it does come with some considerable overhead compared to native C operations.
        
    \item \textbf{Buffer-based communication (\texttt{sum\_ranks\_buffer.py}):} 
    I used the uppercase \texttt{comm.Allreduce} method along with \texttt{numpy} arrays that are typed as integers (\texttt{dtype='i'}). This to avoid Python's overhead by directly passing memory pointers to the MPI C implementation beneath, leading to a much higher performance that's perfect for high-performance computing workloads.
\end{itemize}

\textbf{Verification:} with $P=8$ processes the expected result is the sum of integers from 0 to 7:

\begin{center}
$S = \frac{n(n-1)}{2} = \frac{8 \times 7}{2} = 28$
\end{center}

The output confirms the correctness of both implementations:

\lstinputlisting[
    % language=Python,
    % numbers=left,
    % caption={Welcome message update in main.cpp},
    % captionpos=b,
    % label={lst:mergeCritical},
    % linerange={},
    % firstnumber=111
]{../Skeleton_codes/hpc_python/python_58120.out}

\subsection{Ghost cell exchange between neighboring processes [5 Points]}

In this task, I recreated the 2D Cartesian topology and the ghost cell exchange mechanism using the \texttt{mpi4py} library. This implementation is built on four important functions that help manage the domain decomposition and communication:

\begin{itemize}
    \item \texttt{MPI.Compute\_dims(size, 2)}: Automatically calculates the optimal grid dimensions based on the number of available processes. In my test case with $P=8$, it generated a $4 \times 2$ grid.
    \item \texttt{comm.Create\_cart(dims, periods=[True, True])}: Creates a new communicator with a Cartesian topology. The \texttt{periods} argument enables periodic boundaries (torus) in both vertical and horizontal directions.
    \item \texttt{cart\_comm.Shift(direction, disp=1)}: Queries the topology to determine the ranks of neighbors. It returns the source (neighbor in the negative direction) and destination (neighbor in the positive direction) for data shifting.
    \item \texttt{cart\_comm.sendrecv(...)}: Performs a blocking send and receive operation simultaneously. This is safer than separate blocking send/recv calls as it automatically handles buffering to prevent deadlocks during the cyclic exchange.
\end{itemize}

\textbf{Verification:}
The script was executed with 8 processes, which produced a $4 \times 2$ grid. The output shows that the topological setup and data exchange are correct. For instance, \textbf{Rank 0} (coordinates $[0,0]$) accurately recognizes its North neighbor as \textbf{Rank 6} (coordinates $[3,0]$), demonstrating that the vertical boundary wraps around periodically. Additionally, the verification step confirms that the data received matches the rank of the topological neighbors.

\lstinputlisting[
    language=bash,
    % numbers=left,
    % caption={Welcome message update in main.cpp},
    % captionpos=b,
    % label={lst:mergeCritical},
    % linerange={},
    % firstnumber=111
]{../Skeleton_codes/hpc_python/ghost/python_58125.out}


\subsection{A self-scheduling example: Parallel Mandelbrot [15 Points]}

In this project, I implemented a load-balancing system using \texttt{mpi4py} for the \textit{Manager-Worker} model. This approach works really well for the Mandelbrot set, where the computational load per pixel can differ a lot—like, pixels inside the set need maximum iterations to calculate, but those outside can diverge in no time.

\textbf{Implementation Details:}
The logic is divided based on the MPI rank:
\begin{itemize}
    \item \textbf{Manager (Rank 0):} 
    The system keeps a queue of tasks. It starts by sending one task to each worker with \texttt{comm.send}. Then, it gets into a loop, waiting for \textit{any} worker to finish a task. As soon as it gets a result back, it sends a new task to that worker right away. When there are no more tasks left in the queue, it sends a termination signal \texttt{TAG\_DONE}.
    \item \textbf{Workers (Rank > 0):} 
    Execute an infinite loop waiting for messages. Upon receiving a task, they compute the Mandelbrot patch and send the object back. The loop terminates only upon receiving \texttt{TAG\_DONE}.
\end{itemize}

\textbf{Scaling Analysis:}
As requested the study was done on a $4001 \times 4001$ area using between 2 and 16 workers, looking at two different granularities: 50 tasks and 100 tasks. The results are all in Table~\ref{tab:mandel_scaling}.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Workers} & \textbf{Total Procs} & \textbf{Time (50 Tasks)} & \textbf{Time (100 Tasks)} \\ \hline
2  & 3  & 13.49 s & 13.43 s \\ \hline
4  & 5  & 8.00 s  & 7.51 s \\ \hline
8  & 9  & 4.78 s  & 4.56 s \\ \hline
16 & 17 & 3.56 s  & \textbf{3.21 s} \\ \hline
\end{tabular}
\caption{Runtime comparison for different task granularities ($4001^2$ grid).}
\label{tab:mandel_scaling}
\end{table}

\textbf{Observations:}
\begin{enumerate}
    \item \textbf{Load Balancing Benefits:} 
    The configuration with 100 tasks consistently beats the one with just 50. With 16 workers, breaking it down into 100 tasks gives you $\approx 10\%$ boost in performance 3.21 seconds compared to 3.56 seconds.
    \item \textbf{Granularity Impact:} 
    With only 50 tasks for those 16 workers, each is handling $\approx 3$ patches on average. That can lead to a few workers getting overloaded with tougher tasks, which we call 'stragglers.' But when you bump it up to 100 tasks, with each worker managing avg $\approx 6$, it balances things out. Faster workers can tackle easier patches, while others deal with the harder ones, reducing the chances of everyone just sitting around waiting at the end of the simulation.
\end{enumerate}

\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.37\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/mini_app/output.png}
        \vspace{2mm}
        \caption{ex1 - Nonlinear PDE}
        \label{fig:ex1}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Skeleton_codes/hpc_python/ManagerWorker/mandelbrot.png}
        \vspace{2mm}
        \caption{ex 2.3 - Mandelbrot set}
        \label{fig:ex2_3}
    \end{minipage}
\end{figure}


\newpage
% =============================================================================
\section{Task:  Quality of the Report [15 Points]}
% =============================================================================



\end{document}
