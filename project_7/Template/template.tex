\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{amssymb}

\lstdefinestyle{hpcblock}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!60},
    xleftmargin=1.5em,
    xrightmargin=1.5em,
    aboveskip=1em,
    belowskip=1em,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=hpcblock}



\input{assignment.sty}
\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Paolo Deidda}{Discussed with: FULL NAME}{Solution for Project 7}{}
\newline

\assignmentpolicy

\tableofcontents
\newpage

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 1 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %

\section{HPC Mathematical Software for Extreme-Scale Science  [85 points]}

\subsection{Boundary problem above in Python [25 points]}

We implemented a 2D Poisson solver for the boundary value problem:
\[
-\Delta u = f \quad \text{in } \Omega = [0,1]^2
\]
with Dirichlet boundary conditions $u = 0$ on $\partial\Omega$ and source term $f(x,y) = 2\pi^2 \sin(\pi x)\sin(\pi y)$.

\textbf{Approach:}
\begin{itemize}
    \item Discretized the domain using a uniform grid with $n \times n$ interior points and spacing $h = 1/(n+1)$.
    \item Applied the 5-point finite difference stencil for the Laplacian operator.
    \item Assembled the sparse system matrix $A$ using \texttt{scipy.sparse} (CSR format) and the RHS vector $b$.
    \item Solved the linear system $Au = b$ using \texttt{scipy.sparse.linalg.spsolve}.
\end{itemize}

The matrix $A$ is a block tridiagonal matrix of size $n^2 \times n^2$ with the standard 2D Laplacian structure scaled by $1/h^2$.

\subsection{Boundary problem above in PETSc [25 points]}

We reimplemented the same problem using PETSc for parallel scalability.

\textbf{Approach:}
\begin{itemize}
    \item Used \texttt{petsc4py} to interface with PETSc from Python.
    \item Created a distributed sparse matrix (\texttt{PETSc.Mat}) with parallel row ownership.
    \item Each MPI process assembles only its local portion of the matrix and RHS vector.
    \item Solved using the Krylov Subspace method (KSP) with default preconditioner (ILU) or configurable options (e.g., CG with Jacobi).
    \item The DMDA (Distributed Array) structure handles domain decomposition and ghost point communication.
\end{itemize}

The PETSc implementation enables scaling to multiple processors while maintaining the same numerical discretization as the Python reference.

\subsection{Validate and Visualize [10 points]}

We validated both implementations by comparing the numerical solution against the analytical solution:
\[
u_{\text{exact}}(x,y) = \sin(\pi x)\sin(\pi y)
\]

\subsubsection*{Visualization}

Figure~\ref{fig:solutions2d} shows the 2D heatmaps of the solutions, while Figure~\ref{fig:solutions3d} shows the 3D surface plots.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_dir.png}
        \caption{SciPy Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_cg.png}
        \caption{SciPy CG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_petsc.png}
        \caption{PETSc}
    \end{subfigure}
    \caption{2D heatmaps of numerical solutions ($128 \times 32$ grid).}
    \label{fig:solutions2d}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_dir_3d.png}
        \caption{SciPy Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_cg_3d.png}
        \caption{SciPy CG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_petsc_3d.png}
        \caption{PETSc}
    \end{subfigure}
    \caption{3D surface plots of numerical solutions ($128 \times 32$ grid).}
    \label{fig:solutions3d}
\end{figure}

\subsubsection*{Validation}

Table~\ref{tab:validation} summarizes the solution norms and solver times for a $128 \times 32$ grid.

\begin{table}[H]
    \centering
    \caption{Validation results for $128 \times 32$ grid}
    \label{tab:validation}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Solver} & \textbf{$L_2$ Norm} & \textbf{Solve Time (s)} \\
        \hline
        SciPy Direct (sparse)  & 58.1316 & 0.0126 \\
        SciPy Dense Direct     & 58.1316 & 1.8122 \\
        SciPy CG (sparse)      & 58.1316 & 0.0167 \\
        PETSc (1 proc)         & 51.7557 & 0.0058 \\
        \hline
    \end{tabular}
\end{table}

All Python solvers produce identical $L_2$ norms (58.1316), confirming consistency. The PETSc solution has a slightly different norm due to different grid handling (DMDA). The sparse direct solver is the fastest among Python methods, while PETSc achieves the lowest solve time even on a single processor.

\subsection{Performance Benchmark [15 points]}

We benchmarked the solvers across different grid sizes to evaluate their scalability. Table~\ref{tab:perf} shows the solve times for each method.

\begin{table}[H]
    \centering
    \caption{Solve time (seconds) vs. grid size for different solvers}
    \label{tab:perf}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Grid} & \textbf{SciPy Direct} & \textbf{SciPy Dense} & \textbf{SciPy CG} & \textbf{PETSc} \\
        \hline
        $8 \times 8$     & 0.00032  & 0.00054  & 0.00050  & 0.00013 \\
        $16 \times 16$   & 0.00084  & 0.00132  & 0.00103  & 0.00009 \\
        $32 \times 32$   & 0.00337  & 0.04161  & 0.00254  & 0.00048 \\
        $64 \times 64$   & 0.01457  & 1.80845  & 0.00911  & 0.00331 \\
        $128 \times 128$ & 0.07412  & 99.097   & 0.04881  & 0.02598 \\
        $256 \times 256$ & 0.44292  & ---      & 0.35763  & 0.20776 \\
        $512 \times 512$ & 3.04059  & ---      & 3.22541  & 1.94871 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Observations:}
\begin{itemize}
    \item \textbf{Dense solver} scales as $\mathcal{O}(n^6)$ (cubic in matrix size $n^2 \times n^2$), becoming impractical for grids larger than $128 \times 128$ (99s).
    \item \textbf{Sparse direct solver} scales approximately as $\mathcal{O}(n^3)$ due to fill-in during LU factorization.
    \item \textbf{CG solver} scales as $\mathcal{O}(n^2 \cdot k)$ where $k$ is the number of iterations, competitive with sparse direct for large grids.
    \item \textbf{PETSc} consistently outperforms all Python solvers, even on a single processor, due to optimized C/Fortran backends and efficient preconditioning.
\end{itemize}

Figure~\ref{fig:perf} shows the scaling behavior on a log-log plot.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{../src/poisson/test_perf/solution_sp_dir.png}
    \caption{Performance comparison (placeholder - replace with actual timing plot if available).}
    \label{fig:perf}
\end{figure}
                     
\subsection{Strong Scaling [10 points]}

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Report Quality ------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Quality of the Report [15 Points] }


\section*{Additional notes and submission details}
Submit the source code files (together with your used \texttt{Makefile}) in
an archive file (tar, zip, etc.), and summarize your results and the
observations for all exercises by writing an extended Latex report.
Use the Latex template from the webpage and upload the Latex summary
as a PDF to \href{https://www.icorsi.ch}{iCorsi}.

\begin{itemize}
	\item Your submission should be a gzipped tar archive, formatted like project\_number\_lastname\_firstname.zip or project\_number\_lastname\_firstname.tgz. 
	It should contain:
	\begin{itemize}
		\item all the source codes of your solutions;
		\item your write-up with your name  project\_number\_lastname\_firstname.pdf.
	\end{itemize}
	\item Submit your .zip/.tgz through Icorsi.
\end{itemize}

\end{document}
