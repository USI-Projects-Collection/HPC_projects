\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{amssymb}

\lstdefinestyle{hpcblock}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!60},
    xleftmargin=1.5em,
    xrightmargin=1.5em,
    aboveskip=1em,
    belowskip=1em,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=hpcblock}



\input{assignment.sty}
\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Paolo Deidda}{Discussed with: FULL NAME}{Solution for Project 7}{}
\newline

\assignmentpolicy

\tableofcontents
\newpage

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 1 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %

\section{HPC Mathematical Software for Extreme-Scale Science  [85 points]}

\subsection{Boundary problem above in Python [25 points]}

We implemented a 2D Poisson solver for the boundary value problem:
\[
-\Delta u = f \quad \text{in } \Omega = [0,1]^2
\]
with Dirichlet boundary conditions $u = 0$ on $\partial\Omega$ and source term $f(x,y) = 2\pi^2 \sin(\pi x)\sin(\pi y)$.

\textbf{Approach:}
\begin{itemize}
    \item Discretized the domain using a uniform grid with $n \times n$ interior points and spacing $h = 1/(n+1)$.
    \item Applied the 5-point finite difference stencil for the Laplacian operator.
    \item Assembled the sparse system matrix $A$ using \texttt{scipy.sparse} (CSR format) and the RHS vector $b$.
    \item Solved the linear system $Au = b$ using \texttt{scipy.sparse.linalg.spsolve}.
\end{itemize}

The matrix $A$ is a block tridiagonal matrix of size $n^2 \times n^2$ with the standard 2D Laplacian structure scaled by $1/h^2$.

\subsection{Boundary problem above in PETSc [25 points]}

We reimplemented the same problem using PETSc for parallel scalability.

\textbf{Approach:}
\begin{itemize}
    \item Used \texttt{petsc4py} to interface with PETSc from Python.
    \item Created a distributed sparse matrix (\texttt{PETSc.Mat}) with parallel row ownership.
    \item Each MPI process assembles only its local portion of the matrix and RHS vector.
    \item Solved using the Krylov Subspace method (KSP) with default preconditioner (ILU) or configurable options (e.g., CG with Jacobi).
    \item The DMDA (Distributed Array) structure handles domain decomposition and ghost point communication.
\end{itemize}

The PETSc implementation enables scaling to multiple processors while maintaining the same numerical discretization as the Python reference.

\subsection{Validate and Visualize [10 points]}

We validated both implementations by comparing the numerical solution against the analytical solution:
\[
u_{\text{exact}}(x,y) = \sin(\pi x)\sin(\pi y)
\]

\subsubsection*{Visualization}

Figure~\ref{fig:solutions2d} shows the 2D heatmaps of the solutions, while Figure~\ref{fig:solutions3d} shows the 3D surface plots.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_dir.png}
        \caption{SciPy Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_dn_dir.png}
        \caption{SciPy Dense Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_cg.png}
        \caption{SciPy CG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_petsc.png}
        \caption{PETSc}
    \end{subfigure}
    \caption{2D heatmaps of numerical solutions ($128 \times 32$ grid).}
    \label{fig:solutions2d}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_dir_3d.png}
        \caption{SciPy Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_dn_dir_3d.png}
        \caption{SciPy Dense Direct}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_cg_3d.png}
        \caption{SciPy CG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_petsc_3d.png}
        \caption{PETSc}
    \end{subfigure}
    \caption{3D surface plots of numerical solutions ($128 \times 32$ grid).}
    \label{fig:solutions3d}
\end{figure}

\subsubsection*{Validation}

Table~\ref{tab:validation} summarizes the solution norms and solver times for a $128 \times 32$ grid.

\begin{table}[H]
    \centering
    \caption{Validation results for $128 \times 32$ grid}
    \label{tab:validation}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Solver} & \textbf{$L_2$ Norm} & \textbf{Solve Time (s)} \\
        \hline
        SciPy Direct (sparse)  & 58.1316 & 0.0126 \\
        SciPy Dense Direct     & 58.1316 & 1.8122 \\
        SciPy CG (sparse)      & 58.1316 & 0.0167 \\
        PETSc (1 proc)         & 51.7557 & 0.0058 \\
        \hline
    \end{tabular}
\end{table}

All Python solvers produce identical $L_2$ norms (58.1316), confirming consistency. The PETSc solution has a slightly different norm due to different grid handling (DMDA). The sparse direct solver is the fastest among Python methods, while PETSc achieves the lowest solve time even on a single processor.

\subsection{Performance Benchmark [15 points]}

We benchmarked the solvers across different grid sizes to evaluate their scalability. Table~\ref{tab:perf} shows the solve times for each method.

\begin{table}[H]
    \centering
    \caption{Solve time (seconds) vs. grid size for different solvers}
    \label{tab:perf}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Grid} & \textbf{SciPy Direct} & \textbf{SciPy Dense} & \textbf{SciPy CG} & \textbf{PETSc} \\
        \hline
        $8 \times 8$     & 0.00032  & 0.00054  & 0.00050  & 0.00013 \\
        $16 \times 16$   & 0.00084  & 0.00132  & 0.00103  & 0.00009 \\
        $32 \times 32$   & 0.00337  & 0.04161  & 0.00254  & 0.00048 \\
        $64 \times 64$   & 0.01457  & 1.80845  & 0.00911  & 0.00331 \\
        $128 \times 128$ & 0.07412  & 99.097   & 0.04881  & 0.02598 \\
        $256 \times 256$ & 0.44292  & ---      & 0.35763  & 0.20776 \\
        $512 \times 512$ & 3.04059  & ---      & 3.22541  & 1.94871 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Observations:}
\begin{itemize}
    \item \textbf{Dense solver} scales as $\mathcal{O}(n^6)$ (cubic in matrix size $n^2 \times n^2$), becoming impractical for grids larger than $128 \times 128$ (99s).
    \item \textbf{Sparse direct solver} scales approximately as $\mathcal{O}(n^3)$ due to fill-in during LU factorization.
    \item \textbf{CG solver} scales as $\mathcal{O}(n^2 \cdot k)$ where $k$ is the number of iterations, competitive with sparse direct for large grids.
    \item \textbf{PETSc} consistently outperforms all Python solvers, even on a single processor, due to optimized C/Fortran backends and efficient preconditioning.
\end{itemize}
                     
\subsection{Strong Scaling [10 points]}

We performed a strong scaling study using the PETSc solver with CG method on a fixed problem size of $1024 \times 1024$ grid points, varying the number of MPI processes from 1 to 16.

\begin{table}[H]
    \centering
    \caption{Strong scaling results for PETSc solver ($1024 \times 1024$ grid)}
    \label{tab:scaling}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Processes} & \textbf{Solve Time (s)} & \textbf{Speedup} & \textbf{Efficiency} & \textbf{Mflop/s} \\
        \hline
        1  & 18.680 & 1.00  & 100.0\% & 906  \\
        2  & 10.779 & 1.73  & 86.7\%  & 1779 \\
        4  & 5.306  & 3.52  & 88.0\%  & 3639 \\
        8  & 3.462  & 5.39  & 67.4\%  & 5618 \\
        16 & 1.809  & 10.33 & 64.5\%  & 10676 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Observations:}
\begin{itemize}
    \item The solver achieves a speedup of $10.33\times$ with 16 processes, corresponding to 64.5\% parallel efficiency.
    \item Efficiency remains above 85\% up to 4 processes, then decreases due to communication overhead.
    \item The aggregate throughput increases from 906 Mflop/s (1 proc) to 10,676 Mflop/s (16 procs), showing effective utilization of additional resources.
    \item The sub-linear scaling is expected for iterative solvers due to:
    \begin{itemize}
        \item Global reductions in dot products and norms (VecNorm, VecTDot)
        \item Increased number of CG iterations with more processes (538 $\rightarrow$ 612 iterations)
        \item Communication overhead in matrix-vector products (VecScatter)
    \end{itemize}
\end{itemize}

The strong scaling behavior demonstrates that PETSc provides reasonable parallel efficiency for this problem size, making it suitable for large-scale simulations where single-processor execution time is prohibitive.

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Report Quality ------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Quality of the Report [15 Points] }


\section*{Additional notes and submission details}
Submit the source code files (together with your used \texttt{Makefile}) in
an archive file (tar, zip, etc.), and summarize your results and the
observations for all exercises by writing an extended Latex report.
Use the Latex template from the webpage and upload the Latex summary
as a PDF to \href{https://www.icorsi.ch}{iCorsi}.

\begin{itemize}
	\item Your submission should be a gzipped tar archive, formatted like project\_number\_lastname\_firstname.zip or project\_number\_lastname\_firstname.tgz. 
	It should contain:
	\begin{itemize}
		\item all the source codes of your solutions;
		\item your write-up with your name  project\_number\_lastname\_firstname.pdf.
	\end{itemize}
	\item Submit your .zip/.tgz through Icorsi.
\end{itemize}

\end{document}
