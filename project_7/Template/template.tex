\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{amssymb}

\lstdefinestyle{hpcblock}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!60},
    xleftmargin=1.5em,
    xrightmargin=1.5em,
    aboveskip=1em,
    belowskip=1em,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=hpcblock}



\input{assignment.sty}
\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Paolo Deidda}{Discussed with: FULL NAME}{Solution for Project 7}{}
\newline

\assignmentpolicy

\tableofcontents
\newpage

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Exercise 1 ----------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %

\section{HPC Mathematical Software for Extreme-Scale Science  [85 points]}

\subsection{Boundary problem above in Python [25 points]}

I implemented a 2D Poisson solver for the boundary value problem:
\[
-\Delta u = f \quad \text{in } \Omega = [0,1]^2
\]
with Dirichlet boundary conditions $u = 0$ on $\partial\Omega$ and constant source term $f(x,y) = 20$.

\textbf{Approach:}
\begin{itemize}
    \item Discretized the domain using a uniform grid with $n \times n$ interior points and spacing $h = 1/(n+1)$.
    \item Applied the 5-point finite difference stencil for the Laplacian operator.
    \item Assembled the sparse system matrix $A$ using \texttt{scipy.sparse} (CSR format) and the RHS vector $b$.
    \item Solved the linear system $Au = b$ using \texttt{scipy.sparse.linalg.spsolve}.
\end{itemize}

The matrix $A$ is a block tridiagonal matrix of size $n^2 \times n^2$ with the standard 2D Laplacian structure scaled by $1/h^2$.

\subsection{Boundary problem above in PETSc [25 points]}

I reimplemented the same problem using PETSc for parallel scalability.

\textbf{Approach:}
\begin{itemize}
    \item Used \texttt{petsc4py} to interface with PETSc from Python.
    \item Created a distributed sparse matrix (\texttt{PETSc.Mat}) with parallel row ownership.
    \item Each MPI process assembles only its local portion of the matrix and RHS vector.
    \item Solved using the Krylov Subspace method (KSP) with default preconditioner (ILU) or configurable options (e.g., CG with Jacobi).
    \item The DMDA (Distributed Array) structure handles domain decomposition and ghost point communication.
\end{itemize}

The PETSc implementation enables scaling to multiple processors while maintaining the same numerical discretization as the Python reference.

\newpage
\subsection{Validate and Visualize [10 points]}

I checked both implementations by comparing the numerical results from the Python (SciPy) and PETSc solvers. Since the source term $f(x,y) = 20$ is constant, there isn't a closed-form analytical solution. So, I validated the accuracy by making sure both approaches gave the same results.

\subsubsection*{Visualization}

Figure~\ref{fig:solutions2d} shows the 2D heatmaps of the solutions, while Figure~\ref{fig:solutions3d} shows the 3D surface plots.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_dir.png}
        \caption{SciPy Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_dn_dir.png}
        \caption{SciPy Dense Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_cg.png}
        \caption{SciPy CG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_petsc.png}
        \caption{PETSc}
    \end{subfigure}
    \caption{2D heatmaps of numerical solutions ($128 \times 32$ grid).}
    \label{fig:solutions2d}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_dir_3d.png}
        \caption{SciPy Direct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_dn_dir_3d.png}
        \caption{SciPy Dense Direct}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_sp_cg_3d.png}
        \caption{SciPy CG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../src/poisson/test_val/solution_petsc_3d.png}
        \caption{PETSc}
    \end{subfigure}
    \caption{3D surface plots of numerical solutions ($128 \times 32$ grid).}
    \label{fig:solutions3d}
\end{figure}

\subsubsection*{Validation}

Table~\ref{tab:validation} summarizes the solution norms and solver times for a $128 \times 32$ grid.

\begin{table}[H]
    \centering
    \caption{Validation results for $128 \times 32$ grid}
    \label{tab:validation}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Solver} & \textbf{$L_2$ Norm} & \textbf{Solve Time (s)} \\
        \hline
        SciPy Direct (sparse)  & 51.7557 & 0.0122 \\
        SciPy Dense Direct     & 51.7557 & 1.8334 \\
        SciPy CG (sparse)      & 51.7557 & 0.0161 \\
        PETSc (1 proc)         & 51.7557 & 0.0057 \\
        \hline
    \end{tabular}
\end{table}

All solvers produce identical $L_2$ norms (51.7557), confirming that both Python and PETSc implementations solve the same discretized problem correctly. There's actually a small difference in the last decimal place ($\sim 10^{-6}$) is due to floating-point precision and iterative solver tolerances.

\newpage
\subsection{Performance Benchmark [15 points]}

I benchmarked the solvers across different grid sizes to evaluate their scalability. Table~\ref{tab:perf} shows the solve times for each method.

\begin{table}[H]
    \centering
    \caption{Solve time (seconds) vs. grid size for different solvers}
    \label{tab:perf}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Grid} & \textbf{SciPy Direct} & \textbf{SciPy Dense} & \textbf{SciPy CG} & \textbf{PETSc} \\
        \hline
        $8 \times 8$     & 0.00028  & 0.00052  & 0.00039  & 0.00009 \\
        $16 \times 16$   & 0.00074  & 0.00163  & 0.00093  & 0.00013 \\
        $32 \times 32$   & 0.00301  & 0.04268  & 0.00251  & 0.00051 \\
        $64 \times 64$   & 0.01456  & 1.82628  & 0.00850  & 0.00334 \\
        $128 \times 128$ & 0.08201  & 99.487   & 0.04782  & 0.02628 \\
        $256 \times 256$ & 0.55354  & ---      & 0.35962  & 0.20802 \\
        $512 \times 512$ & 4.71116  & ---      & 3.19826  & 2.04333 \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../src/poisson/test_perf/benchmark_loglog.png}
    \caption{Log-log plot of solve time vs. grid size for different solvers. Reference slopes $\mathcal{O}(n^2)$ and $\mathcal{O}(n^3)$ are shown.}
    \label{fig:benchmark}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item \textbf{Dense solver} scales as $\mathcal{O}(n^6)$ (cubic in matrix size $n^2 \times n^2$), becoming impractical for grids larger than $128 \times 128$ (99s).
    \item \textbf{Sparse direct solver} scales approximately as $\mathcal{O}(n^3)$ due to fill-in during LU factorization.
    \item \textbf{CG solver} scales as $\mathcal{O}(n^2 \cdot k)$ where $k$ is the number of iterations, competitive with sparse direct for large grids.
    \item \textbf{PETSc} consistently outperforms all Python solvers, even on a single processor, due to optimized C/Fortran backends and efficient preconditioning.
\end{itemize}
                     
\subsection{Strong Scaling [10 points]}

The strong scaling study used the PETSc solver with CG method on a fixed problem size of $1024 \times 1024$ grid points, varying the number of MPI processes from 1 to 16.

\begin{table}[H]
    \centering
    \caption{Strong scaling results for PETSc solver ($1024 \times 1024$ grid)}
    \label{tab:scaling}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Processes} & \textbf{Solve Time (s)} & \textbf{Speedup} & \textbf{Efficiency} & \textbf{Mflop/s} \\
        \hline
        1  & 18.470 & 1.00  & 100.0\% & 917   \\
        2  & 9.901  & 1.87  & 93.3\%  & 1937  \\
        4  & 5.335  & 3.46  & 86.6\%  & 3619  \\
        8  & 3.474  & 5.32  & 66.4\%  & 5598  \\
        16 & 1.820  & 10.15 & 63.4\%  & 10610 \\
        \hline
    \end{tabular}
\end{table}

The strong scaling behavior demonstrates that PETSc provides reasonable parallel efficiency for this problem size, making it suitable for large-scale simulations where single-processor execution time is prohibitive.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{../src/poisson/test_scale/scaling_loglog.png}
    \caption{Strong scaling results}
    \label{fig:scaling}
\end{figure}

% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% --- Report Quality ------------------------------------------------------- %
% -------------------------------------------------------------------------- %
% -------------------------------------------------------------------------- %
\newpage
\section{Task: Quality of the Report [15 Points] }


\section*{Additional notes and submission details}
Submit the source code files (together with your used \texttt{Makefile}) in
an archive file (tar, zip, etc.), and summarize your results and the
observations for all exercises by writing an extended Latex report.
Use the Latex template from the webpage and upload the Latex summary
as a PDF to \href{https://www.icorsi.ch}{iCorsi}.

\begin{itemize}
	\item Your submission should be a gzipped tar archive, formatted like project\_number\_lastname\_firstname.zip or project\_number\_lastname\_firstname.tgz. 
	It should contain:
	\begin{itemize}
		\item all the source codes of your solutions;
		\item your write-up with your name  project\_number\_lastname\_firstname.pdf.
	\end{itemize}
	\item Submit your .zip/.tgz through Icorsi.
\end{itemize}

\end{document}
